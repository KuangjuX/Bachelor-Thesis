% !Mode:: "TeX:UTF-8"

\titlecontents{chapter}[2em]{\vspace{.5\baselineskip}\xiaosan\song}
             {\prechaptername\CJKnumber{\thecontentslabel}\postchaptername\qquad}{}
             {}             % 设置该选项为空是为了不让目录中显示页码
\addcontentsline{toc}{chapter}{中文译文}
\setcounter{page}{1}            % 单独从 1 开始编页码
\markboth{中文译文}{中文译文}   % 用于将章节号添加到页眉中
%\captionsetup[figure]{labelfont={bf},labelformat={default},labelsep=period,name={Fig.}}
\chapter*{中文译文}

虚拟化是一种广泛应用的技术推动因素，从云计算和服务器到移动和嵌入式系统 [1]。作为云计算的基本基石，虚拟化为工作负载管理、数据保护和成本/功率效率提供了众多优势 [2]。 另一方面，嵌入式和安全关键系统行业已将虚拟化作为一种基本方法来应对市场压力，以最大限度地减少尺寸、重量、功率和成本 (SWaP-C)，同时保证时间和空间 认证隔离（例如，ISO26262）[3]-[5]。 由于虚拟化在多个行业和用例中的激增，硅行业的知名企业开始在主流计算架构中引入硬件虚拟化支持（例如，分别为英特尔虚拟化技术、Arm 虚拟化扩展）[6]、[7]。

计算机体系结构的最新进展带来了一种名为 RISC-V [8] 的新型指令集架构 (ISA)。 RISC-V 最近达到了一百亿个出口的大关 [9]。 它通过提供免费和开放的标准 ISA 将自己与经典主流区分开来，具有模块化和高度可定制的扩展方案，使其能够从小型微控制器扩展到超级计算机 [10]-[16]。 RISC-V 特权架构通过定义 Hypervisor 扩展 [17] 为虚拟化提供硬件支持，于 2021 年第四季度获得批准。

尽管 H 扩展已经获得了批准，但截至撰写本文时，在市场上仍然没有支持 H 扩展的芯片。有上游支持 Hypervisor 扩展的开源 hypervisor，例如 Bao [1]、Xvisor [18]、KVM [19] 和 seL4 [20]（Xen [21] 和 Jailhouse [22] 正在进行中])。然而，据我们所知，只有少数硬件实现部署在 FPGA 上，包括 Rocket 芯片 [23] 和 NOEL-V [24]（很快还有 SHAKTI 和 Chromite [25]）。 尽管如此，现在还没有 (i) 专注于理解和增强虚拟化的微体系结构，以及 (ii) 执行设计空间探索 (DSE) 和伴随的功率、性能、面积 (PPA) 分析的工作。

在这项工作中，我们描述了基于开源 RISC-V CVA6 [14]（64 位）SoC 中虚拟化的架构和微架构支持。 在体系结构层面，该实现符合 Hypervisor 扩展 (v1.0) [17]，并且还包括 RISC-V 计时器 (Sstc) 扩展 [26] 的实现。 在微架构层面，我们修改了 CVA6 微架构以支持 Hypervisor 扩展，并提出了一组额外的扩展/增强功能以减少硬件虚拟化开销：(i) L1 TLB，(ii) 专用第二阶段 TLB PTW（即我们术语中的 GTLB），以及 (iii) L2 TLB。 我们还介绍并讨论了对微体系结构的综合设计空间探索。 我们首先评估部署在 FPGA (Genesys 2) 上的 23 个（共 288 个）硬件设计，并评估对功能性能（执行周期）和硬件的影响。 然后，我们选择 10 种设计，并通过 22nm FDX 技术实现的后仿真技术对其进行深入分析。

总而言之，通过这项工作，我们做出了以下贡献。 首先，我们在 CVA6 内核中提供硬件虚拟化支持。 尤其是我们为嵌套内存管理单元 (MMU) 设计了一组（面向虚拟化的）微体系结构增强功能（第 三节）。 据我们所知，没有任何工作或研究描述和讨论微架构扩展以改进 RISC-V 内核中的硬件虚拟化支持。 其次，我们进行了关于设计空间的探索 (DSE)，其中包含数十种设计配置。 该 DSE 包括权衡来自三个不同微架构组件（L1 TLB、GTLB、L2 TLB）的参数以及各自对功能性能和硬件成本的影响（第四节）。 最后，我们对一些选定的设计配置进行布局后仿真，以评估功率、性能和面积 (PPA) 分析（第五节）。

对于 DSE 评估，我们运行了 MiBench（automotive 的子集）基准测试来评估功能性能。 未优化的实现虚拟化科扩展的 CVA6 作为我们的基准配置。 软件启动包括一个运行在 Bao 虚拟机管理程序之上的 Linux 虚拟机，用于单核设计。 我们测量了托管 Linux VM 相对于基准配置的性能加速。 DSE 探索的结果表明，所提出的微架构扩展可以实现高达 19% 的功能性能加速（例如，对于 susanc 小型基准）； 然而，在某些情况下，相对于基准，面积和功率的增加不可忽略。 因此，PPA 分析的结果表明：(i) Sstc 扩展对功率和面积的影响可以忽略不计； (iii) GTLB 增加总面积不到 1%； (iv) L2 TLB 在某些配置中引入了不可忽略的 8% 面积增加。 作为具有代表性的平衡配置，我们选择了支持 Sstc 的 CVA6 设计和具有 8 个条目的 GTLB。 对于这种特定的硬件配置，我们观察到性能加速高达 16%（平均约 12.5%），而面积和功耗的成本分别为 0.78% 和 0.33%。 据我们所知，本文报告了针对虚拟化增强型 RISC-V 核心的完整 DSE 评估和 PPA 分析的第一项公开工作。

\section*{背景}

\subsection*{A. RISC-V 特权级标准}

RISC-V 特权指令集架构 (ISA) [17] 将其执行模型分为 3 个特权级别：(i) Machine 模式（M-Mode）是最高特权级别，托管实现  supervisor 二进制接口的固件 (SBI)（例如，OpenSBI）； (ii) Supervisor 模式 (S-Mode) 运行需要虚拟内存管理的 Unix 类型操作系统 (OSes)； (iii) User 模式（U-Mode）执行用户态应用程序。 RISC-V ISA 提供的模块化无缝地允许在不同的设计点实现，范围从仅支持 M 模式的小型嵌入式平台到具有 M/S/U 的成熟服务器类系统。

\subsection*{B. CVA6}

CVA6（以前称为Ariane）是一个应用级 RISC-V 软核，它实现了RISC-V ISA [14]的RV64和RV32两个版本。该核完全支持M/S/U三种特权执行模式，并通过实现内存管理单元（MMU），提供了内存虚拟化的硬件支持，使其适合运行一个完整的操作系统，如Linux。最近的新增功能还包括一个节能的向量单元协处理器[27]。在内部，CVA6微架构包含了一个六级流水线，单发射，带有乱序执行阶段和8个PMP条目。MMU有分开的数据和指令TLB，而页表遍历单元（PTW）实现了特权规范[17]定义的Sv39和Sv32转换模式。

\subsection*{C. RISC-V 虚拟化}

与其他主流 ISA 不同，RISC-V 特权架构从最初的概念就被设计为可传统虚拟化 [28]。 因此，尽管 ISA 本身允许直接实现 hypervisor，例如采用经典虚拟化技术 （例如，陷入并模拟和影子页表技术），但众所周知，此类技术会导致严重的性能下降并且无法应对当前的嵌入式实时虚拟化要求（例如，中断延迟）[23]。 因此，为了提高虚拟化效率，RISC-V 特权架构规范通过（可选）管理程序扩展 [17] 引入了对虚拟化的硬件支持。

\textbf{特权级.} 如图 1 所示，RISC-V 管理程序扩展执行模型遵循正交设计，其中 supervisor 模式（S 模式）被修改为 hypervisor-extended supervisor 模式（HS 模式），非常适合托管两种类型 -1 或类型 2 管理程序2。 此外，还添加了两种新的特权模式，可用于在 virtual supervisor 模式（VS 模式）和 virtual user 模式（VU 模式）下运行 guest 操作系统。

\textbf{两阶段地址翻译.} Hypervisor 扩展还定义了第二阶段地址转换（RISC-V 术语中的 G 阶段），通过将客户物理地址 (GPA) 转换为主机物理地址 (HPA) 来虚拟化客户内存。HS 模式像 S 模式一样运行，但有额外的 hypervisor 寄存器和指令来控制虚拟机执行和 G 阶段转换。 例如，hgatp 寄存器保存 G 阶段根页表地址和相应的翻译特定配置字段。

\textbf{Hypervisor 控制状态寄存器(CSRs).} 在 VS 模式下运行的每个虚拟机都有自己的控制状态寄存器 (CSR)，它们是 S 模式 CSR 的影子副本。 这些寄存器可用于确定客户机执行状态和执行虚拟机切换。 为了控制虚拟化状态，使用了一个称为虚拟化模式（V 位）的特定标志。 当V=1时，guest 在 VS-mode 或 VU-mode 下执行，正常的 S-mode CSRs访问实际上是在访问 VS-mode CSRs，并且 G-stage translation 开启。 否则，如果 V=0，则正常的 S 模式 CSR 开启，并且 G 阶段被禁用。 为了简化与 guest 相关的异常陷阱处理，存在特定于来宾的陷阱，例如，guest 页面错误、VS 级异常和 VS 级 ecall（又名 hypercalls）。

\subsection*{D. Nested-MMU}

MMU 是一个硬件组件，负责将虚拟内存转换为物理内存，同时强制进行内存访问权限检查。 操作系统通过为每个进程分配虚拟地址空间并管理 MMU 转换结构来控制 MMU，以便将虚拟地址 (VA) 正确转换为物理地址 (PA)。 在虚拟化系统上，MMU 可以将客户虚拟地址 (GVA) 转换为客户物理地址 (GPA)，并将 GPA 转换为主机物理地址 (HPA)。 在这种情况下，此功能称为嵌套 MMU。 RISC-V ISA 通过将 GPA 转换为 HPA 的新翻译阶段支持嵌套 MMU，表示为 G 阶段。 guest VM 控制翻译的第一阶段（RISC-V 术语中的 VS 阶段），而 hypervisor 控制第二阶段（G 阶段）。 最初，RISC-V 特权规范定义了通过使用四种不同拓扑之一遍历多级基数树表将 VA 转换为 PA：(i) Sv32 用于 32位虚拟地址空间 (VAS)，具有 2级层层级树； (ii) 39 位虚拟地址空间 Sv39，具有 3 级树； (iii) 48 位虚拟地址空间和 4 级树的 Sv48； (iv) 用于 57 位虚拟地址空间和 5 级树的 Sv57。 每个级别都有一个指向下一个表（非叶条目）或最终翻译（叶条目）的指针。 该指针和相应的权限存储在 64 位 (RV64) 或 32 位 (RV32) 宽度的页表条目 (PTE) 中。 请注意，RISC-V 将虚拟地址拆分为 4KiB 页面大小，但由于每个级别都可以将非叶子节点作为叶子节点，因此它支持超页以减少 TLB 压力，例如，Sv39 支持 4KiB、2MiB 和 1GiB 页面大小。

\subsection*{E. RISC-V "stimercmp/vstimecmp" 扩展(Sstc)}

RISC-V Sstc 扩展 [26] 旨在通过其自身的定时器中断设施增强 supervisor 模式，从而消除模拟 S/HS 模式定时器和在 M 模式下产生定时器中断的大量开销。 Sstc 扩展还为 VS 模式的 Hypervisor 扩展添加了类似的功能。 为了能够在 HS/S 模式和 VS 模式下直接控制定时器中断，Sstc 包含两个额外的 CSR：(i) stimecmp 和 (ii) vstimecmp。 每当时间计数器寄存器的值大于 stimecmp 的值时，监控定时器中断挂起 (STIP) 位变为高电平并且定时器中断被传送到 HS/S 模式。 VS 模式也是如此，只有一个细微差别：guest delta 寄存器 (htimedelta) 的偏移量被添加到时间值中。 如果 vstimecmp 大于 time+htimedelta，则 VSTIP 变为 1 并产生 VS 模式定时器中断。 有关 RISC-V 定时器架构的完整概述以及关于为什么经典 RISC-V 定时器规范会导致显着性能损失的讨论，我们建议有兴趣的读者参阅 [23]。

\section*{CVA6 Hypervisor 支持：体系结构和微体系结构}

在本节中，我们描述了 CVA6 中的架构和微架构硬件虚拟化支持（符合 RISC-V Hypervisor 扩展 v1.0），如图 2 所示。

\subsection*{A. Hypervisor 和 Virtual Supervisor 执行模式}

如前所述（参见第 II-C 节），Hypervisor 扩展规范将 S 模式扩展为 HS 模式，并添加了两个额外的正交执行模式，分别表示为 VS 模式和 VU 模式。 为了增加对这种新执行模式的支持，我们扩展/修改了一些 CVA6 核心功能块，特别是 CSR 和解码模块。 如图 2 所示，硬件虚拟化架构逻辑包含五个构建块：(i) VS 模式和 HS 模式 CSR 访问逻辑和权限检查； (ii) 异常和中断的触发和代理； (iii) 陷阱的进入和退出； (iv) hypervisor 指令解码和执行； (v) 嵌套 MMU 翻译逻辑。 CSR 模块被扩展以实现构成硬件虚拟化逻辑的前三个构建块，具体而言：(i) HS 模式和 VS 模式 CSR 访问逻辑（读/写操作）； (ii) HS 和 VS 执行模式陷阱进入和返回逻辑； (iii) 从 M 模式到 HS 模式和/或 VS/VU 模式的异常/中断触发和代理逻辑的一部分（例如，当 VTM 位在 hstatus CSR 中被设置时，读/写 vsatp CSR 在 VS 模式下触发异常）。 修改解码模块以实现 hypervisor 指令解码（例如，hypervisor 加载/存储指令和 memory fence 指令）和所有 VS 模式相关指令执行访问异常触发。

我们建议读者参阅表 I，其中总结了已完全和部分实现的功能。 我们已经实现了已批准的 1.0 版规范的所有必要性功能； 然而，由于对即将到来的或更新的扩展的依赖，我们仍然保留了一些可选功能作为部分实现。 例如：hvencfg 位依赖于 Zicbom [29]（缓存块管理操作）； hgeie 和 hgeip 依赖于 Advanced Interrupt Architucture (AIA) [30]； 和 hgatp 依赖于 vanilla CVA6 当前不支持的虚拟地址空间。

\subsection*{B. Hypervisor Load/Store 指令}

Hypervisor 加载/存储指令（即 HLV、HSV 和 HLVX）为 hypervisor 提供了一种机制来访问 guest 内存，同时接受与 VS 和 VU 模式中相同的地址翻译和权限检查。 这些指令在指令粒度级别更改地址翻译，强制完全交换特权级别和在每个 hypervisor 加载/存储指令执行时应用的翻译。 该实现包括向 CVA6 管道添加信号（在图 2 中标识为 hyp ld/st），该管道从解码行进到执行阶段的加载/存储单元。 然后将该信号送入 MMU，MMU 执行 (i) 所有必要的上下文切换（即启用 hgatp 和 vstap CSR），(ii) 启用虚拟化模式，以及 (iii) 更改 hstatus.SPVP 中指定的执行模式字段。

\subsection*{C. 嵌套页表遍历(PTW)}

MMU 的主要功能块之一是页表遍历器 (PTW)。 从根本上说，PTW 负责根据特定的拓扑和方案（例如 Sv39）对虚拟地址进行划分，然后使用内存页表结构将其转换为物理地址。 Hypervisor 扩展指定了一个新的转换阶段（G 阶段），用于将客户物理地址转换为主机物理地址。 我们的实现支持裸机转换模式（无 G 阶段）和 Sv39x4，它定义了一个 41 位宽的最大客户物理地址空间（CVA6 已经支持的虚拟空间）。

我们扩展了现有的用于将虚拟地址翻译成物理地址的有限状态机 (FSM)，并仅添加了一个新的控制状态来跟踪翻译的阶段并帮助 VS-Stage 和 G-Stage 翻译之间的上下文切换。 使用 G 翻译，必须翻译 (i) VS-Stage 翻译产生的叶 GPA，以及 (ii) VS-Stage 遍历期间使用的所有非叶 PTE GPA。 为实现这一目标，我们确定了 PTW 迭代期间可能发生的三个转换阶段：(i) VS-Stage——PTW 当前状态正在将 guest VA 转换为 GPA； (ii) G-Stage Intermed——PTW 目前的状态是将非叶页表项 GPA 转化为 HPA； (iii) G-Stage Final——PTW 当前状态正在将最终输出地址从 VS-Stage 转换为 HPA。 值得注意的是，如果 hgatp 处于 Bare 模式，则没有 G-stage 转换，我们执行标准的 VS-Stage 转换。 嵌套遍历完成后，PTW 使用最终的 PTE  VS 阶段和 G 阶段以及当前地址空间标识符 (ASID) 和 VMID 更新 TLB。 一项已经实现的优化包括将 VS 和 G 阶段的翻译页面大小（即 4KiB、2MiB 和 1GiB）存储到同一个 TLB 条目中，以及每个阶段的权限访问位。 这有助于减少 TLB 命中时间并提高硬件重用率。

\subsection*{D. Virtualization-aware TLBs(vTLB)}

CVA6 MMU 微架构有两个小的全关联 TLB：一个用于数据 (L1 DTLB)，另一个用于指令 (L1 ITLB)。 两个 TLB 都最多支持 16 个条目并完全实现了刷新指令，即 sfence，包括按 ASID 和虚拟地址过滤。 为了支持嵌套翻译，我们修改了 L1 DTLB 和 ITLB 的微体系结构以支持两阶段翻译，包括访问权限和 VMID。 每个 TLB 条目都有 VS-Stage 和 G-Stage PTE 以及各自的权限。 查找逻辑的实现是使用来自两个阶段结果的合并最终翻译大小执行的，即如果 VS 阶段是 4KiB 而 G 阶段是 2MiB，则最终翻译的结果将是 4KiB。 这可能是将 VS 阶段和 G 阶段存储在一起的主要缺点之一。 例如，支持超页/大页的 hypervisor 使用 2MiB 页面大小来优化 MMU 性能。 尽管这会极大减少了 PTW 遍历时间，但如果 guest 使用 4KiB 页面大小，TLB 查找将不会从超页中获益，因为页表翻译将会转化为 4KiB 页面大小。 另一种方法是为 VS 阶段和 G 阶段的最终转换使用单独的 TLB，但这会转化为更高的硬件成本以及较少的硬件可重用（如果 G 阶段未激活）以及更高的性能损失如果 L1 TLB 命中 的话（TLB 搜索时间增加了大约 2 倍）。 由于 L1 TLB 是位于 CPU 关键路径上的完全组合电路，我们决定将 VS 阶段和 G 阶段转换保留在单个 TLB 条目中。 最后，TLB 还支持 VMID 标签，允许 TLB 管理程序可以使用 per-VMID 刷新执行更高效的 TLB 管理，并避免在 VM 上下文切换时完全刷新 TLB。 最后一点，TLB 还允许通过 guest 物理地址进行刷新，即完全支持 hypervisor fence 指令 (HFENCE.VVMA/GVMA)。

\subsection*{E. 微体系结构扩展 \#1 - GTLB}

Sv39 方案的完整嵌套表遍历最多需要 15 次内存访问，是标准 S 阶段翻译的五倍。 这种额外的负担会带来更高的 TLB 未命中惩罚，导致功能性能（执行周期）的（测量的）开销高达 13\%
（将基准虚拟化实现与普通 CVA6 进行比较）。 为了缓解这种情况，我们在嵌套 PTW 模块中使用 G-Stage TLB (GTLB) 扩展了 CVA6 微架构，以存储中间 GPA 到 HPA 的转换，即 VS-Stage 非叶子 PTE guest 物理地址到 host 物理地址翻译。 图 3 说明了将 GTLB 集成到 MMU 微体系结构中所需的修改。 GTLB 结构旨在通过在 VS 阶段页表遍历期间跳过每个嵌套翻译来加速 VS 阶段翻译。 图 4 显示了一个 4KiB 页面大小的转换过程，在 PTW 中具有 GTLB。 如果没有 GTLB，guest 每次在遍历过程中形成非叶子物理地址页表项指针时，它需要：(i) 通过 G-Stage 转换它，(ii) 从内存中读取下一级 PTE 值，以及 (iii) 恢复 VS 阶段翻译。 使用超页（2MiB 或 1GiB）时，翻译路径更少，从而降低了性能损失。 使用简单的硬件结构，可以通过缓存这些 G 阶段翻译来减轻此类开销，同时避免不必要的缓存污染和不确定的内存访问。 支持这种方法的另一个原因与 PTE 在内存中按顺序打包在一起的事实有关，即多个 VS-Stage 非叶 PTE 地址转换将共享相同的 GTLB 条目 [31]。











